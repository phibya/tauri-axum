{
  "model": "test-llama",
  "messages": [
    {
      "role": "user", 
      "content": "Hi, how are you?"
    }
  ],
  "max_tokens": 50,
  "temperature": 0.3
}